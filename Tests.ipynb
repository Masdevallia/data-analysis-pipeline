{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages and loading env:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "#..............................................................................................\n",
    "\n",
    "# Importing the 3 dataframes:\n",
    "df_1star = pd.read_csv('./input/one-star-michelin-restaurants.csv')\n",
    "df_2star = pd.read_csv('./input/two-stars-michelin-restaurants.csv')\n",
    "df_3star = pd.read_csv('./input/three-stars-michelin-restaurants.csv')\n",
    "\n",
    "# Adding column 'stars':\n",
    "df_1star['stars'] = [1]*df_1star.shape[0]\n",
    "df_2star['stars'] = [2]*df_2star.shape[0]\n",
    "df_3star['stars'] = [3]*df_3star.shape[0]\n",
    "\n",
    "# Putting the three dataframes together:\n",
    "df = pd.concat([df_1star, df_2star,df_3star], ignore_index=True, sort=False)\n",
    "\n",
    "# Deleting columns that I will not need in my program: Award year & zipCode:\n",
    "df = df.drop(['zipCode','year'], axis=1)\n",
    "\n",
    "#..............................................................................................\n",
    "\n",
    "# Types:\n",
    "# print(df.dtypes) # All correct\n",
    "\n",
    "#..............................................................................................\n",
    "\n",
    "# Missing values:\n",
    "null_cols = df.isnull().sum()\n",
    "# print(null_cols[null_cols > 0])\n",
    "# city: 2\n",
    "# price: 176\n",
    "\n",
    "#..............................................................................................\n",
    "\n",
    "# City:\n",
    "\n",
    "# Web scraping in https://guide.michelin.com to get the cities\n",
    "city_list = []\n",
    "for e in df['url'][df['city'].isnull() == True]:\n",
    "    url = e\n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    city_list.append(re.sub('\\n|\\s','', soup.select('.restaurant-details__heading--list')[0].text).split(',')[-2][:8])\n",
    "\n",
    "# Filling NaN values:\n",
    "index_city = 0\n",
    "for i in df[df['city'].isnull() == True].index:\n",
    "    df.at[i,'city'] = '{} {}'.format(city_list[index_city][:4],city_list[index_city][4:])\n",
    "    index_city += 1\n",
    "\n",
    "# df['city'].isnull().sum() # 0\n",
    "    \n",
    "#..............................................................................................\n",
    "\n",
    "# Price:\n",
    "\n",
    "# Web scraping in https://guide.michelin.com to get the prices    \n",
    "prices_rest_list = []\n",
    "for i in range(len(df)):\n",
    "    url = df['url'][i]\n",
    "    res = requests.get(url)\n",
    "    html = res.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    prices = soup.select('.restaurant-details__heading-price')\n",
    "    if prices:\n",
    "        # prices_list.append([df['name'][i],re.sub('\\n|\\s','',prices[0].text)])\n",
    "        prices_rest_list.append([df['name'][i], re.sub('\\n|\\s','',prices[0].text).split('•')[0]])\n",
    "\n",
    "restaurants = [e[0] for e in prices_rest_list]\n",
    "price = [e[1] for e in prices_rest_list]\n",
    "\n",
    "def resub_list(array, sub_before, sub_after):\n",
    "    import re\n",
    "    return [re.sub(sub_before,sub_after,e) for e in array]\n",
    "\n",
    "# Deleting thousands separator:\n",
    "correct_price = resub_list(price,',','')\n",
    "\n",
    "# Some restaurants don't have currency information. I'm deleting them.\n",
    "rows_to_delete = [bool(re.match('[A-Z]{3}', correct_price[i][-3:])) for i in range(len(correct_price))]\n",
    "restaurants = [restaurants[i] for i in range(len(restaurants)) if rows_to_delete[i] == True]\n",
    "correct_price = [correct_price[i] for i in range(len(correct_price)) if rows_to_delete[i] == True]\n",
    "\n",
    "# Separating price values and currency:\n",
    "correct_price_2 = []\n",
    "for i in range(len(correct_price)):\n",
    "    correct_price_2.append('{} {}'.format(correct_price[i][:-3],correct_price[i][-3:]))\n",
    "    \n",
    "correct_price_3 = list(map(lambda x: x.split(' '), correct_price_2))\n",
    "\n",
    "price = [e[0] for e in correct_price_3]\n",
    "currency = [e[1] for e in correct_price_3]\n",
    "price_minmax = list(map(lambda x: x.split('-'), price))\n",
    "price_min = [int(e[0]) for e in price_minmax]\n",
    "price_max = [int(e[1]) for e in price_minmax]\n",
    "\n",
    "# Changing all prices to EUR:\n",
    "\n",
    "def exchangerate_api_request(currency):\n",
    "    url = \"https://api.exchangerate-api.com/v4/latest/{}\".format(currency)\n",
    "    res = requests.get(url)\n",
    "    return res\n",
    "\n",
    "exchangerate = exchangerate_api_request('EUR').json()\n",
    "\n",
    "currencies_to_change = list(set([e for e in currency if e != 'EUR']))\n",
    "# for e in currencies_to_change:\n",
    "    # if e not in list(exchangerate['rates'].keys()):\n",
    "        # print(e) # MOP\n",
    "\n",
    "# MOP TO HKD:\n",
    "url = 'https://en.wikipedia.org/wiki/Macanese_pataca'\n",
    "res = requests.get(url)\n",
    "html = res.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "mop_hkd = float(soup.select('#mw-content-text > div > table:nth-child(1) > tbody > tr:nth-child(26) > td')[0].text[-4:])\n",
    "# 'Hong Kong dollar (HK$) HK$1 = MOP$1.03'\n",
    "\n",
    "price_min_eur = []\n",
    "price_max_eur = []\n",
    "i = 0\n",
    "for e in currency:\n",
    "    if e == 'MOP':\n",
    "        price_min_eur.append(price_min[i]/mop_hkd/exchangerate['rates']['HKD'])  \n",
    "        price_max_eur.append(price_max[i]/mop_hkd/exchangerate['rates']['HKD'])\n",
    "    else:\n",
    "        price_min_eur.append(price_min[i]/exchangerate['rates'][e])\n",
    "        price_max_eur.append(price_max[i]/exchangerate['rates'][e])\n",
    "    i += 1\n",
    "\n",
    "# print(min(price_min_eur[0:10])) # 33.0\n",
    "# print(max(price_min_eur[0:10])) # 92.0\n",
    "# print(np.mean(price_min_eur[0:10])) # 64.5\n",
    "# print(min(price_max_eur[0:10])) # 78.0\n",
    "# print(max(price_max_eur[0:10])) # 158.0\n",
    "# print(np.mean(price_max_eur[0:10])) # 116.8\n",
    "\n",
    "df['min_price_EUR'] = [np.nan]*len(df)\n",
    "df['max_price_EUR'] = [np.nan]*len(df)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    for j in range(len(restaurants)):\n",
    "        if df.at[i,'name'] == restaurants[j]:\n",
    "            df.at[i,'min_price_EUR'] = price_min_eur[j]\n",
    "            df.at[i,'max_price_EUR'] = price_max_eur[j]\n",
    "\n",
    "# print(df['max_price_EUR'].isnull().sum()) # 26\n",
    "# print(df['min_price_EUR'].isnull().sum()) # 26\n",
    "\n",
    "# Deleting column 'price':\n",
    "df = df.drop(['price'], axis=1)\n",
    "\n",
    "# Deleting rows with missing 'price' values\n",
    "df_final = df[~df['max_price_EUR'].isnull()]\n",
    "df_final.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#..............................................................................................\n",
    "\n",
    "# Missing values:\n",
    "null_cols = df_final.isnull().sum()\n",
    "# print(null_cols[null_cols > 0])\n",
    "\n",
    "#..............................................................................................\n",
    "\n",
    "# Standardizing 'cuisine' column:\n",
    "\n",
    "for i in range(len(df_final)):\n",
    "    df_final.at[i,'cuisine'] = df_final.at[i,'cuisine'].capitalize()\n",
    "\n",
    "#..............................................................................................\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final.to_csv('./output/cleaned_enriched_df.csv')\n",
    "df_final = pd.read_csv('./output/cleaned_enriched_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Austria',\n",
       " 'California',\n",
       " 'Chicago',\n",
       " 'Croatia',\n",
       " 'Czech Republic',\n",
       " 'Denmark',\n",
       " 'Finland',\n",
       " 'Greece',\n",
       " 'Hong Kong',\n",
       " 'Hungary',\n",
       " 'Ireland',\n",
       " 'Macau',\n",
       " 'New York City',\n",
       " 'Norway',\n",
       " 'Poland',\n",
       " 'Rio de Janeiro',\n",
       " 'Sao Paulo',\n",
       " 'Singapore',\n",
       " 'South Korea',\n",
       " 'Sweden',\n",
       " 'Taipei',\n",
       " 'Thailand',\n",
       " 'United Kingdom',\n",
       " 'Washington DC'}"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_final['region']) # La web los clasifica según estas regiones, pero no se corresponden con paises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def battuta_request_authorized(resource):\n",
    "    authToken = os.getenv(\"BATTUTA_API_KEY\")\n",
    "    if not authToken:\n",
    "        raise ValueError(\"Missing API key\")\n",
    "    else:\n",
    "        print(\"Battuta API key: \", authToken[0:4], '[...]')\n",
    "    url = \"http://battuta.medunes.net/api{}key={}\".format(resource,authToken)\n",
    "    res = requests.get(url)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Battuta API key:  c243 [...]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'remaining quota': 224}"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "battuta_request_authorized('/quota/?').json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = list(set(df['region']))\n",
    "regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De las regiones, saco las que sí son paises:\n",
    "data3 = []\n",
    "for e in regions:\n",
    "    data3.append([e,battuta_request_authorized('/country/search/?country={}&'.format(e)).json()])\n",
    "data3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country'] = ['country']*len(df)\n",
    "\n",
    "countries = []\n",
    "for i in range(len(data3)):\n",
    "    if len(data3[i][1]) > 0:\n",
    "        countries.append(data3[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    for country in countries:\n",
    "        if df.at[i,'region'] == country:\n",
    "            df.at[i, 'country'] = country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_regions = [e[0] for e in data3 if e[0] not in countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estas últimas no encuentro otra manera de hacerlas que manualmente...\n",
    "\n",
    "remaining_regions_searching = [remaining_regions[0][:8], remaining_regions[1], remaining_regions[2][6:], \\\n",
    "                               remaining_regions[3], remaining_regions[4], 'London', remaining_regions[6], \\\n",
    "                               'Illinois',remaining_regions[8][:-3],remaining_regions[9][:-1]]\n",
    "\n",
    "remaining_countries = []\n",
    "\n",
    "for e in remaining_regions_searching:\n",
    "    remaining_countries.append(battuta_request_authorized('/country/search/?city={}&'.format(e)).json()[0]['name'])\n",
    "    remaining_countries.append(battuta_request_authorized('/country/search/?city={}&'.format(e)).json()[0]['name'])\n",
    "    remaining_countries.append(battuta_request_authorized('/country/search/?country={}&'.format(e)).json()[1]['name'])\n",
    "    remaining_countries.append(battuta_request_authorized('/country/search/?region={}&'.format(e)).json()[1]['name'])\n",
    "    remaining_countries.append(battuta_request_authorized('/country/search/?city={}&'.format(e)).json()[0]['name'])\n",
    "    remaining_countries.append(battuta_request_authorized('/country/search/?city={}&'.format(e)).json()[0]['name'])\n",
    "    remaining_countries.append(battuta_request_authorized('/country/search/?city={}&'.format(e)).json()[0]['name'])\n",
    "    remaining_countries.append(battuta_request_authorized('/country/search/?region={}&'.format(e)).json()[0]['name'])\n",
    "    remaining_countries.append(battuta_request_authorized('/country/search/?city={}&'.format(e)).json()[1]['name'])\n",
    "    remaining_countries.append(battuta_request_authorized('/country/search/?country={}&'.format(e)).json()[0]['name'])\n",
    "\n",
    "remaining_countries[2] = remaining_regions[2]\n",
    "remaining_countries[5] = remaining_regions[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    for j in range(len(remaining_regions)):\n",
    "        if df.at[i,'region'] == remaining_regions[j]:\n",
    "            df.at[i, 'country'] = remaining_countries[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Ireland', 'Taiwan', 'Croatia', 'Sweden', 'Thailand', 'Greece', 'South Korea', 'United States of America', 'Macao', 'Norway', 'United Kingdom', 'Hong Kong', 'Brazil', 'Austria', 'Hungary', 'Finland', 'Singapore', 'Poland', 'Denmark', 'Czech Republic'}\n"
     ]
    }
   ],
   "source": [
    "print(set(df['country']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('./output/test_restaurants_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('./output/test_countries.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
