{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages and loading env:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from src.api import exchangerate_api_request\n",
    "from src.api import battuta_request_authorized\n",
    "from src.webscraping import get_soup\n",
    "from src.clean import resub_list\n",
    "\n",
    "# Importing the 3 dataframes:\n",
    "df_1star = pd.read_csv('./input/one-star-michelin-restaurants.csv')\n",
    "df_2star = pd.read_csv('./input/two-stars-michelin-restaurants.csv')\n",
    "df_3star = pd.read_csv('./input/three-stars-michelin-restaurants.csv')\n",
    "\n",
    "# Adding column 'stars':\n",
    "df_1star['stars'] = [1]*df_1star.shape[0]\n",
    "df_2star['stars'] = [2]*df_2star.shape[0]\n",
    "df_3star['stars'] = [3]*df_3star.shape[0]\n",
    "\n",
    "# Putting the three dataframes together:\n",
    "df = pd.concat([df_1star, df_2star,df_3star], ignore_index=True, sort=False)\n",
    "\n",
    "# Deleting columns that I will not need in my program: Award year & zipCode:\n",
    "df = df.drop(['zipCode','year'], axis=1)\n",
    "\n",
    "# Filling NaN values in 'city':\n",
    "# Web scraping https://guide.michelin.com to get cities\n",
    "city_list = []\n",
    "for e in df['url'][df['city'].isnull() == True]:\n",
    "    soup = get_soup(e)\n",
    "    city_list.append(re.sub('\\n|\\s','', soup.select('.restaurant-details__heading--list')[0].text).split(',')[-2][:8])\n",
    "# Filling NaN values:\n",
    "index_city = 0\n",
    "for i in df[df['city'].isnull() == True].index:\n",
    "    df.at[i,'city'] = '{} {}'.format(city_list[index_city][:4],city_list[index_city][4:])\n",
    "    index_city += 1\n",
    "    \n",
    "# Filling NaN values in 'price':\n",
    "# Web scraping https://guide.michelin.com to get prices\n",
    "prices_rest_list = []\n",
    "for i in range(len(df)):\n",
    "    soup = get_soup(df['url'][i])\n",
    "    prices = soup.select('.restaurant-details__heading-price')\n",
    "    if prices:\n",
    "        prices_rest_list.append([df['name'][i], re.sub('\\n|\\s','',prices[0].text).split('•')[0]])\n",
    "restaurants = [e[0] for e in prices_rest_list]\n",
    "price = [e[1] for e in prices_rest_list]\n",
    "\n",
    "# Deleting thousands separator:\n",
    "correct_price = resub_list(price,',','')\n",
    "\n",
    "# Some restaurants don't have currency information. I'm deleting them.\n",
    "rows_to_delete = [bool(re.match('[A-Z]{3}', correct_price[i][-3:])) for i in range(len(correct_price))]\n",
    "restaurants = [restaurants[i] for i in range(len(restaurants)) if rows_to_delete[i] == True]\n",
    "correct_price = [correct_price[i] for i in range(len(correct_price)) if rows_to_delete[i] == True]\n",
    "\n",
    "# Separating min price values, max price values and currency:\n",
    "correct_price_2 = []\n",
    "for i in range(len(correct_price)):\n",
    "    correct_price_2.append('{} {}'.format(correct_price[i][:-3],correct_price[i][-3:]))\n",
    "correct_price_3 = list(map(lambda x: x.split(' '), correct_price_2))\n",
    "price = [e[0] for e in correct_price_3]\n",
    "currency = [e[1] for e in correct_price_3]\n",
    "price_minmax = list(map(lambda x: x.split('-'), price))\n",
    "price_min = [int(e[0]) for e in price_minmax]\n",
    "price_max = [int(e[1]) for e in price_minmax]\n",
    "\n",
    "# Changing all prices to EUR using an API:\n",
    "exchangerate = exchangerate_api_request('EUR').json()\n",
    "currencies_to_change = list(set([e for e in currency if e != 'EUR']))\n",
    "# for e in currencies_to_change:\n",
    "    # if e not in list(exchangerate['rates'].keys()):\n",
    "        # print(e) # MOP\n",
    "# The API doesn't support MOP: MOP TO HKD through Web Scraping:\n",
    "soup = get_soup('https://en.wikipedia.org/wiki/Macanese_pataca')\n",
    "mop_hkd = float(soup.select('#mw-content-text > div > table:nth-child(1) > tbody > tr:nth-child(26) > td')[0].text[-4:])\n",
    "# 'HKD 1 = MOP 1.03'\n",
    "\n",
    "price_min_eur = []\n",
    "price_max_eur = []\n",
    "i = 0\n",
    "for e in currency:\n",
    "    if e == 'MOP':\n",
    "        price_min_eur.append(price_min[i]/mop_hkd/exchangerate['rates']['HKD'])  \n",
    "        price_max_eur.append(price_max[i]/mop_hkd/exchangerate['rates']['HKD'])\n",
    "    else:\n",
    "        price_min_eur.append(price_min[i]/exchangerate['rates'][e])\n",
    "        price_max_eur.append(price_max[i]/exchangerate['rates'][e])\n",
    "    i += 1\n",
    "\n",
    "df['min_price_EUR'] = [np.nan]*len(df)\n",
    "df['max_price_EUR'] = [np.nan]*len(df)\n",
    "\n",
    "for i in range(len(df)):\n",
    "    for j in range(len(restaurants)):\n",
    "        if df.at[i,'name'] == restaurants[j]:\n",
    "            df.at[i,'min_price_EUR'] = price_min_eur[j]\n",
    "            df.at[i,'max_price_EUR'] = price_max_eur[j]\n",
    "            \n",
    "# Deleting column 'price':\n",
    "df = df.drop(['price'], axis=1)\n",
    "\n",
    "# Deleting rows with missing 'price' values\n",
    "df_final = df[~df['max_price_EUR'].isnull()]\n",
    "df_final.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Deleting otliers:\n",
    "\n",
    "df_price = df_final[['min_price_EUR','max_price_EUR']]\n",
    "\n",
    "stats = df_price.describe().transpose()\n",
    "stats['IQR'] = stats['75%'] - stats['25%']\n",
    "\n",
    "outliers = pd.DataFrame(columns=df_price.columns)\n",
    "for col in stats.index:\n",
    "    iqr = stats.at[col,'IQR']\n",
    "    cutoff = iqr * 15 # Multiplico por 15 porque me interesa sacar sólo los outliers muy muy desorbitados\n",
    "    lower = stats.at[col,'25%'] - cutoff\n",
    "    upper = stats.at[col,'75%'] + cutoff\n",
    "    results = df_price[(df_price[col] < lower) | \n",
    "                   (df_price[col] > upper)].copy()\n",
    "    results['Outlier'] = col\n",
    "    outliers = outliers.append(results)\n",
    "\n",
    "rowstodelete = list(set(outliers.index))\n",
    "\n",
    "df_final.drop(rowstodelete, axis = 0, inplace=True)\n",
    "df_final.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Once the NaNs deleted, I can set the column type to 'int':\n",
    "df_final['min_price_EUR'] = df_final['min_price_EUR'].astype('int')\n",
    "df_final['max_price_EUR'] = df_final['max_price_EUR'].astype('int')\n",
    "\n",
    "# Standardizing 'cuisine' column:\n",
    "for i in range(len(df_final)):\n",
    "    df_final.at[i,'cuisine'] = df_final.at[i,'cuisine'].capitalize()\n",
    "\n",
    "# Standardizing 'city' column:\n",
    "for i in range(len(df_final['city'])):\n",
    "    if len(re.findall('Paulo', df_final.at[i,'city'])) > 0:\n",
    "        df_final.at[i,'city'] = re.sub(df_final.at[i,'city'],'São Paulo',df_final.at[i,'city'])\n",
    "for i in range(len(df_final['city'])):\n",
    "    if len(re.findall('Janeiro', df_final.at[i,'city'])) > 0:\n",
    "        df_final.at[i,'city'] = re.sub(df_final.at[i,'city'],'Rio de Janeiro',df_final.at[i,'city'])\n",
    "        \n",
    "# Classifying regions into countries (according to battuta API):\n",
    "# battuta_request_authorized('/quota/?').json()\n",
    "\n",
    "regions = list(set(df_final['region']))\n",
    "\n",
    "battuta_states = []\n",
    "for e in regions:\n",
    "    battuta_states.append([e,battuta_request_authorized('/country/search/?country={}&'.format(e)).json()])\n",
    "\n",
    "df_final['state'] = ['state']*len(df_final)\n",
    "\n",
    "states = []\n",
    "for i in range(len(battuta_states)):\n",
    "    if len(battuta_states[i][1]) > 0:\n",
    "        states.append(battuta_states[i][0])\n",
    "        \n",
    "for i in range(len(df_final)):\n",
    "    for country in states:\n",
    "        if df_final.at[i,'region'] == country:\n",
    "            df_final.at[i, 'state'] = country\n",
    "\n",
    "remaining_regions = [e[0] for e in battuta_states if e[0] not in states]\n",
    "\n",
    "# Due to the particularities and limitations of the API (max 500 requests), I have to make these last substitutions a bit manually...:\n",
    "remaining_states = []\n",
    "for e in remaining_regions:\n",
    "    if e == 'California':\n",
    "        remaining_states.append(battuta_request_authorized('/country/search/?region={}&'.format(e)).json()[1]['name'])\n",
    "    elif len(battuta_request_authorized('/country/search/?city={}&'.format(e)).json()) > 0:\n",
    "        remaining_states.append(battuta_request_authorized('/country/search/?city={}&'.format(e)).json()[0]['name'])\n",
    "    else:\n",
    "        remaining_states.append(e)\n",
    "\n",
    "remaining_states[4] = 'United States of America'\n",
    "remaining_states[6] = 'United States of America'\n",
    "remaining_states[7] = 'United States of America'\n",
    "\n",
    "for i in range(len(df_final)):\n",
    "    for j in range(len(remaining_regions)):\n",
    "        if df_final.at[i,'region'] == remaining_regions[j]:\n",
    "            df_final.at[i, 'state'] = remaining_states[j]\n",
    "\n",
    "# df_final.drop_duplicates() # 665 rows: There are not duplicates\n",
    "\n",
    "cols = ['name','state','region','city','latitude','longitude','cuisine','stars','min_price_EUR','max_price_EUR','url']\n",
    "df_final = df_final[cols]\n",
    "\n",
    "# df_final.to_csv('./input/cleaned_enriched_df.csv', index=False)\n",
    "# df_final = pd.read_csv('./input/cleaned_enriched_df.csv')\n",
    "\n",
    "# display(df_final.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
